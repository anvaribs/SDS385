{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4ff3d94c-1657-43da-873e-6608a8f48aa4"
    }
   },
   "source": [
    "NMF is best used with the fit_transform method, which returns the matrix W. The matrix H is stored into the fitted model in the components_ attribute; the method transform will decompose a new matrix X_new based on these stored components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "b81662cf-d8c2-4f13-b6d1-c409e467c1c8"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=2, init='random', random_state=0)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_\n",
    "X_new = np.array([[1, 0], [1, 6.1], [1, 0], [1, 4], [3.2, 1], [0, 4]])\n",
    "W_new = model.transform(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "968a37d1-edf7-4ba5-87c1-208989f9681b"
    }
   },
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6751ada0-bd10-4962-b954-eb7407d8e8d1"
    }
   },
   "outputs": [],
   "source": [
    "W_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing NMF implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = nmf.NMF()\n",
    "\n",
    "# create a random sparse matrix\n",
    "A = nmf.rand_sparse_matrix(100000, 1000, 0.01)\n",
    "nmf.setup(A, k = 10)\n",
    "nmf.run(iter_num = 100)\n",
    "\n",
    "# hard clustering\n",
    "nmf.clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nmf.H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nmf.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test sparse NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1     2     3     4     5     6     7     8     9     10    ...   1555  \\\n",
      "1     NaN   NaN   NaN   NaN   NaN   5.0   NaN   NaN   NaN   3.0  ...    NaN   \n",
      "2     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "3     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "4     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "5     4.0   3.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "6     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "7     NaN   NaN   NaN   NaN   NaN   NaN   5.0   5.0   NaN   4.0  ...    NaN   \n",
      "8     NaN   NaN   NaN   NaN   NaN   NaN   3.0   NaN   NaN   NaN  ...    NaN   \n",
      "9     NaN   NaN   NaN   NaN   NaN   5.0   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "10    4.0   NaN   NaN   4.0   NaN   NaN   4.0   NaN   NaN   NaN  ...    NaN   \n",
      "11    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   5.0   NaN  ...    NaN   \n",
      "12    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "13    NaN   3.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "14    NaN   NaN   NaN   NaN   NaN   NaN   5.0   NaN   4.0   NaN  ...    NaN   \n",
      "15    1.0   NaN   NaN   NaN   NaN   NaN   1.0   NaN   4.0   NaN  ...    NaN   \n",
      "16    NaN   NaN   NaN   NaN   NaN   NaN   5.0   5.0   5.0   NaN  ...    NaN   \n",
      "17    4.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   3.0   NaN  ...    NaN   \n",
      "18    5.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "19    NaN   NaN   NaN   4.0   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "20    3.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "21    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "22    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "23    5.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "24    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   5.0   NaN  ...    NaN   \n",
      "25    NaN   NaN   NaN   NaN   NaN   NaN   NaN   4.0   NaN   NaN  ...    NaN   \n",
      "26    NaN   NaN   NaN   NaN   NaN   NaN   3.0   NaN   4.0   NaN  ...    NaN   \n",
      "27    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "28    NaN   NaN   NaN   NaN   NaN   NaN   5.0   NaN   NaN   NaN  ...    NaN   \n",
      "29    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "30    NaN   NaN   NaN   NaN   NaN   NaN   4.0   NaN   NaN   NaN  ...    NaN   \n",
      "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
      "430   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   4.0  ...    NaN   \n",
      "431   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "432   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "433   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "434   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "435   NaN   NaN   NaN   4.0   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "436   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "437   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "438   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "439   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "440   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "442   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "444   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "445   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "446   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "447   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "448   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "449   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "450   NaN   4.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "451   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "452   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "453   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "454   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "455   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "456   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "457   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "458   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "459   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "460   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   3.0  ...    NaN   \n",
      "462   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "\n",
      "     1557  1561  1562  1563  1565  1578  1582  1586  1591  \n",
      "1     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "2     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "3     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "4     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "5     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "6     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "7     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "8     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "9     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "10    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "11    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "12    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "13    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "14    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "15    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "16    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "17    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "18    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "19    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "20    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "21    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "22    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "23    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "24    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "25    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "26    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "27    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "28    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "29    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "30    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "..    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "430   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "431   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "432   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "433   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "434   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "435   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "436   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "437   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "438   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "439   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "440   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "442   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "444   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "445   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   4.0  \n",
      "446   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "447   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "448   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "449   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "450   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "451   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "452   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "453   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "454   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "455   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "456   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "457   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "458   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "459   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "460   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "462   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "\n",
      "[459 rows x 1410 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1     2     3     4     5     6     7     8     9     10    ...   1624  \\\n",
      "2     4.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   2.0  ...    NaN   \n",
      "3     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "4     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "5     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "8     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "9     NaN   NaN   NaN   NaN   NaN   NaN   4.0   NaN   NaN   NaN  ...    NaN   \n",
      "10    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   4.0   NaN  ...    NaN   \n",
      "12    NaN   NaN   NaN   5.0   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "14    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "15    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "16    5.0   NaN   NaN   5.0   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "17    NaN   NaN   NaN   NaN   NaN   NaN   4.0   NaN   NaN   NaN  ...    NaN   \n",
      "19    NaN   NaN   NaN   NaN   NaN   NaN   NaN   5.0   NaN   NaN  ...    NaN   \n",
      "20    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "21    5.0   NaN   NaN   NaN   2.0   NaN   5.0   NaN   5.0   NaN  ...    NaN   \n",
      "22    NaN   2.0   NaN   5.0   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "23    NaN   NaN   NaN   NaN   NaN   NaN   4.0   4.0   NaN   NaN  ...    NaN   \n",
      "24    NaN   NaN   NaN   NaN   NaN   NaN   4.0   5.0   NaN   NaN  ...    NaN   \n",
      "25    5.0   NaN   NaN   NaN   NaN   NaN   4.0   NaN   NaN   NaN  ...    NaN   \n",
      "26    3.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "27    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   4.0   NaN  ...    NaN   \n",
      "28    NaN   NaN   NaN   NaN   3.0   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "29    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "30    NaN   3.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "31    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "32    NaN   NaN   NaN   NaN   NaN   NaN   4.0   NaN   3.0   NaN  ...    NaN   \n",
      "33    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "34    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "35    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "36    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
      "893   5.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "895   4.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "898   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "900   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   2.0   NaN  ...    NaN   \n",
      "902   5.0   NaN   NaN   NaN   NaN   NaN   NaN   5.0   NaN   NaN  ...    NaN   \n",
      "904   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   4.0   NaN  ...    NaN   \n",
      "905   NaN   NaN   NaN   NaN   NaN   NaN   4.0   NaN   NaN   NaN  ...    NaN   \n",
      "906   NaN   NaN   NaN   NaN   NaN   NaN   3.0   NaN   4.0   4.0  ...    NaN   \n",
      "908   NaN   NaN   NaN   NaN   NaN   NaN   3.0   NaN   NaN   NaN  ...    NaN   \n",
      "909   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "910   4.0   NaN   2.0   NaN   NaN   NaN   NaN   NaN   4.0   NaN  ...    NaN   \n",
      "911   NaN   NaN   NaN   NaN   NaN   NaN   4.0   NaN   NaN   NaN  ...    NaN   \n",
      "912   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "914   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "915   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "917   3.0   NaN   1.0   NaN   NaN   NaN   NaN   NaN   5.0   NaN  ...    NaN   \n",
      "920   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "923   3.0   NaN   4.0   NaN   NaN   NaN   NaN   NaN   4.0   NaN  ...    NaN   \n",
      "924   5.0   3.0   NaN   NaN   NaN   4.0   4.0   NaN   4.0   NaN  ...    NaN   \n",
      "925   NaN   NaN   NaN   NaN   4.0   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "926   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "928   NaN   NaN   NaN   NaN   NaN   NaN   NaN   5.0   5.0   NaN  ...    NaN   \n",
      "929   3.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "930   3.0   NaN   NaN   NaN   NaN   NaN   NaN   3.0   NaN   NaN  ...    NaN   \n",
      "931   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "935   3.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   1.0   NaN  ...    NaN   \n",
      "937   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   5.0   NaN  ...    NaN   \n",
      "939   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   5.0   NaN  ...    NaN   \n",
      "941   5.0   NaN   NaN   NaN   NaN   NaN   4.0   NaN   NaN   NaN  ...    NaN   \n",
      "942   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    NaN   \n",
      "\n",
      "     1625  1628  1652  1653  1654  1656  1662  1664  1671  \n",
      "2     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "3     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "4     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "5     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "8     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "9     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "10    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "12    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "14    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "15    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "16    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "17    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "19    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "20    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "21    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "22    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "23    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "24    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "25    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "26    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "27    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "28    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "29    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "30    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "31    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "32    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "33    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "34    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "35    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "36    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "..    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "893   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "895   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "898   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "900   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "902   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "904   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "905   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "906   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "908   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "909   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "910   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "911   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "912   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "914   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "915   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "917   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "920   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "923   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "924   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "925   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "926   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "928   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "929   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "930   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "931   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "935   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "937   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "939   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "941   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "942   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "\n",
      "[655 rows x 1324 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06116678  0.01310999  0.03007978 ...,  0.10167242  0.07702705\n",
      "   0.01885278]\n",
      " [ 0.06074929  0.03600561  0.01109164 ...,  0.04374073  0.05141819\n",
      "   0.03099225]\n",
      " [ 0.03579904  0.04452073  0.0080528  ...,  0.08147691  0.03538561\n",
      "   0.01467281]\n",
      " ..., \n",
      " [ 0.02278584  0.03969371  0.00861663 ...,  0.02725434  0.03812336\n",
      "   0.00349809]\n",
      " [ 0.04399466  0.08661241  0.06507395 ...,  0.01806021  0.00600995\n",
      "   0.02509836]\n",
      " [ 0.02774676  0.00954209  0.00189742 ...,  0.03371126  0.02642919\n",
      "   0.05937389]]\n",
      "[[ 0.04375315  0.0122183   0.07144895 ...,  0.00406109  0.05890683\n",
      "   0.02810835]\n",
      " [ 0.01230577  0.0106495   0.02719706 ...,  0.03240609  0.0059006\n",
      "   0.02117425]\n",
      " [ 0.00883515  0.03332961  0.08697233 ...,  0.03420699  0.03272402\n",
      "   0.11022778]\n",
      " ..., \n",
      " [ 0.0370424   0.01508106  0.06216588 ...,  0.01077181  0.0194017\n",
      "   0.10346756]\n",
      " [ 0.0347152   0.03057598  0.02173061 ...,  0.0216371   0.0020546\n",
      "   0.04770594]\n",
      " [ 0.01140359  0.05319619  0.01655697 ...,  0.01925587  0.06412444\n",
      "   0.01512917]]\n",
      "1\n",
      "350294.939825\n",
      "2\n",
      "312320.697723\n",
      "3\n",
      "247329.269203\n",
      "4\n",
      "174318.803523\n",
      "5\n",
      "125062.381075\n",
      "6\n",
      "96153.2538188\n",
      "7\n",
      "78073.8953937\n",
      "8\n",
      "66091.5452099\n",
      "9\n",
      "57708.8818914\n",
      "10\n",
      "51589.6471979\n",
      "11\n",
      "46969.3330808\n",
      "12\n",
      "43384.6747601\n",
      "13\n",
      "40541.3865907\n",
      "14\n",
      "38244.4245151\n",
      "15\n",
      "36359.6906098\n",
      "16\n",
      "34792.1374145\n",
      "17\n",
      "33472.7008296\n",
      "18\n",
      "32350.1604255\n",
      "19\n",
      "31385.8692513\n",
      "20\n",
      "30550.2274574\n",
      "21\n",
      "29820.2589638\n",
      "22\n",
      "29177.9100255\n",
      "23\n",
      "28608.8336195\n",
      "24\n",
      "28101.5080233\n",
      "25\n",
      "27646.5893625\n",
      "26\n",
      "27236.4303342\n",
      "27\n",
      "26864.7184457\n",
      "28\n",
      "26526.2011828\n",
      "29\n",
      "26216.4750918\n",
      "30\n",
      "25931.822334\n",
      "31\n",
      "25669.08286\n",
      "32\n",
      "25425.5535681\n",
      "33\n",
      "25198.9081014\n",
      "34\n",
      "24987.1325706\n",
      "35\n",
      "24788.4736745\n",
      "36\n",
      "24601.3965481\n",
      "37\n",
      "24424.6418849\n",
      "38\n",
      "24256.9554165\n",
      "39\n",
      "24097.2607587\n",
      "40\n",
      "23944.6556354\n",
      "41\n",
      "23798.2676748\n",
      "42\n",
      "23657.31672\n",
      "43\n",
      "23521.1031165\n",
      "44\n",
      "23388.9819042\n",
      "45\n",
      "23260.3699452\n",
      "46\n",
      "23134.8043556\n",
      "47\n",
      "23011.8171457\n",
      "48\n",
      "22891.1124635\n",
      "49\n",
      "22772.1493935\n",
      "50\n",
      "22654.6895065\n",
      "51\n",
      "22538.2795112\n",
      "52\n",
      "22422.5282347\n",
      "53\n",
      "22307.2293836\n",
      "54\n",
      "22191.9650049\n",
      "55\n",
      "22076.5273336\n",
      "56\n",
      "21960.7093843\n",
      "57\n",
      "21844.3110297\n",
      "58\n",
      "21727.2884771\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import csv\n",
    "import sparse_nmf as smf\n",
    "from sklearn.utils.extmath import randomized_svd, squared_norm\n",
    "\n",
    "\n",
    "file_path = \"../sparse_NMF/ml-100k/u1.test\"\n",
    "f = open(file_path)\n",
    "rows = csv.reader(f)\n",
    "dic = {}\n",
    "for row in rows:\n",
    "    e = row[0].split('\\t')\n",
    "    if not int(e[0]) in dic:\n",
    "#     if not dic.has_key(int(e[0])):\n",
    "        dic[int(e[0])] = {}\n",
    "    dic[int(e[0])][int(e[1])] =int(e[2])\n",
    "temp = []\n",
    "for key in dic:\n",
    "    s = pd.Series(dic[key],name=key)\n",
    "    temp.append(s)\n",
    "t = pd.DataFrame(temp)\n",
    "print (t)\n",
    "t.to_csv('tna.csv')\n",
    "t0 = t.fillna(0)\n",
    "t0.to_csv('t.csv')\n",
    "\n",
    "\n",
    "file_path = \"../sparse_NMF/ml-100k/u1.base\"\n",
    "# file_path = \"C:/Users/Wu/Desktop/ml-1m/ratings.dat\"\n",
    "\n",
    "f = open(file_path)\n",
    "rows = csv.reader(f)\n",
    "\n",
    "dic = {}\n",
    "for row in rows:\n",
    "    e = row[0].split('\\t')\n",
    "    if not int(e[0]) in dic:\n",
    "#     if not dic.has_key(int(e[0])):\n",
    "        dic[int(e[0])] = {}\n",
    "    dic[int(e[0])][int(e[1])] =int(e[2])\n",
    "\n",
    "temp = []\n",
    "for key in dic:\n",
    "    if dic[key].__len__() > 100:\n",
    "        continue\n",
    "\n",
    "    s = pd.Series(dic[key],name=key)\n",
    "    temp.append(s)\n",
    "\n",
    "# print (s)\n",
    "b = pd.DataFrame(temp)\n",
    "print (b)\n",
    "b.to_csv('bna.csv')\n",
    "b0 = b.fillna(0)\n",
    "b0.to_csv('b.csv')\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(b0)\n",
    "mf = smf.Sparse_NMF(n_components=50, max_iter=4000, tol=1000, alpha=0.002, beta=0.001, init='random')\n",
    "W, H = mf.fit(X)\n",
    "print (mf.n_iter_)\n",
    "# bob = np.array([5, 5, 0, 0, 0, 5])\n",
    "# bob.shape = 1,6\n",
    "# w = mf.transform(bob)\n",
    "\n",
    "# W, H = sss_mf(RATE_MATRIX)\n",
    "\n",
    "# nmf = NMF(n_components=10, max_iter=500 )  # 设有2个隐主题\n",
    "# W = nmf.fit_transform(np.array(f))\n",
    "# H = nmf.components_\n",
    "\n",
    "print (W)\n",
    "print (H)\n",
    "recon = pd.DataFrame(np.dot(W,H),b0.index,b0.columns)\n",
    "recon[recon<1] = 1\n",
    "recon[recon>5] = 5\n",
    "# recon.to_csv('mf100recon.csv')\n",
    "print (recon)\n",
    "d = (t0 - recon) * (t0>0)\n",
    "# d.to_csv('mf100d.csv')\n",
    "d.fillna(0,inplace = True)\n",
    "print (squared_norm(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing sparse PCA implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "21c12aed-480c-482c-accb-05d22d75afb2"
    }
   },
   "source": [
    "### Beta divergence loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "670e3603-b567-406f-b30c-800b4900a682"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition.nmf import _beta_divergence\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "x = np.linspace(0.001, 4, 1000)\n",
    "y = np.zeros(x.shape)\n",
    "\n",
    "colors = 'mbgyr'\n",
    "for j, beta in enumerate((0., 0.5, 1., 1.5, 2.)):\n",
    "    for i, xi in enumerate(x):\n",
    "        y[i] = _beta_divergence(1, xi, 1, beta)\n",
    "    name = \"beta = %1.1f\" % beta\n",
    "    plt.plot(x, y, label=name, color=colors[j])\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.title(\"beta-divergence(1, x)\")\n",
    "plt.legend(loc=0)\n",
    "plt.axis([0, 4, 0, 3])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d2db3ce9-1fa8-478b-b751-3f3702c71901"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from time import time\n",
    "\n",
    "from numpy.random import RandomState\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import decomposition\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "n_row, n_col = 2, 3\n",
    "n_components = n_row * n_col\n",
    "image_shape = (64, 64)\n",
    "rng = RandomState(0)\n",
    "\n",
    "# #############################################################################\n",
    "# Load faces data\n",
    "dataset = fetch_olivetti_faces(shuffle=True, random_state=rng)\n",
    "faces = dataset.data\n",
    "\n",
    "n_samples, n_features = faces.shape\n",
    "\n",
    "# global centering\n",
    "faces_centered = faces - faces.mean(axis=0)\n",
    "\n",
    "# local centering\n",
    "faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n",
    "\n",
    "print(\"Dataset consists of %d faces\" % n_samples)\n",
    "\n",
    "\n",
    "def plot_gallery(title, images, n_col=n_col, n_row=n_row):\n",
    "    plt.figure(figsize=(2. * n_col, 2.26 * n_row))\n",
    "    plt.suptitle(title, size=16)\n",
    "    for i, comp in enumerate(images):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        vmax = max(comp.max(), -comp.min())\n",
    "        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,\n",
    "                   interpolation='nearest',\n",
    "                   vmin=-vmax, vmax=vmax)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n",
    "\n",
    "# #############################################################################\n",
    "# List of the different estimators, whether to center and transpose the\n",
    "# problem, and whether the transformer uses the clustering API.\n",
    "estimators = [\n",
    "    ('Eigenfaces - PCA using randomized SVD',\n",
    "     decomposition.PCA(n_components=n_components, svd_solver='randomized',\n",
    "                       whiten=True),\n",
    "     True),\n",
    "\n",
    "    ('Non-negative components - NMF',\n",
    "     decomposition.NMF(n_components=n_components, init='nndsvda', tol=5e-3),\n",
    "     False),\n",
    "\n",
    "    ('Independent components - FastICA',\n",
    "     decomposition.FastICA(n_components=n_components, whiten=True),\n",
    "     True),\n",
    "\n",
    "    ('Sparse comp. - MiniBatchSparsePCA',\n",
    "     decomposition.MiniBatchSparsePCA(n_components=n_components, alpha=0.8,\n",
    "                                      n_iter=100, batch_size=3,\n",
    "                                      random_state=rng),\n",
    "     True),\n",
    "\n",
    "    ('MiniBatchDictionaryLearning',\n",
    "        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,\n",
    "                                                  n_iter=50, batch_size=3,\n",
    "                                                  random_state=rng),\n",
    "     True),\n",
    "\n",
    "    ('Cluster centers - MiniBatchKMeans',\n",
    "        MiniBatchKMeans(n_clusters=n_components, tol=1e-3, batch_size=20,\n",
    "                        max_iter=50, random_state=rng),\n",
    "     True),\n",
    "\n",
    "    ('Factor Analysis components - FA',\n",
    "     decomposition.FactorAnalysis(n_components=n_components, max_iter=2),\n",
    "     True),\n",
    "]\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Plot a sample of the input data\n",
    "\n",
    "plot_gallery(\"First centered Olivetti faces\", faces_centered[:n_components])\n",
    "\n",
    "# #############################################################################\n",
    "# Do the estimation and plot it\n",
    "\n",
    "for name, estimator, center in estimators:\n",
    "    print(\"Extracting the top %d %s...\" % (n_components, name))\n",
    "    t0 = time()\n",
    "    data = faces\n",
    "    if center:\n",
    "        data = faces_centered\n",
    "    estimator.fit(data)\n",
    "    train_time = (time() - t0)\n",
    "    print(\"done in %0.3fs\" % train_time)\n",
    "    if hasattr(estimator, 'cluster_centers_'):\n",
    "        components_ = estimator.cluster_centers_\n",
    "    else:\n",
    "        components_ = estimator.components_\n",
    "\n",
    "    # Plot an image representing the pixelwise variance provided by the\n",
    "    # estimator e.g its noise_variance_ attribute. The Eigenfaces estimator,\n",
    "    # via the PCA decomposition, also provides a scalar noise_variance_\n",
    "    # (the mean of pixelwise variance) that cannot be displayed as an image\n",
    "    # so we skip it.\n",
    "    if (hasattr(estimator, 'noise_variance_') and\n",
    "            estimator.noise_variance_.ndim > 0):  # Skip the Eigenfaces case\n",
    "        plot_gallery(\"Pixelwise variance\",\n",
    "                     estimator.noise_variance_.reshape(1, -1), n_col=1,\n",
    "                     n_row=1)\n",
    "    plot_gallery('%s - Train time %.1fs' % (name, train_time),\n",
    "                 components_[:n_components])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "25fa3973-6dc7-4d24-a3f1-5778f89d1922"
    }
   },
   "source": [
    "## NMF vs LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ff35af81-12bf-4e47-aca0-f1bd09c2504d"
    }
   },
   "source": [
    "This is an example of applying NMF and Latent Dirichlet Allocation on a corpus of documents and extract additive models of the topic structure of the corpus. The output is a list of topics, each represented as a list of terms (weights are not shown).\n",
    "\n",
    "\n",
    "Non-negative Matrix Factorization is applied with two different objective functions: the Frobenius norm, and the generalized Kullback-Leibler divergence. The latter is equivalent to Probabilistic Latent Semantic Indexing.\n",
    "\n",
    "\n",
    "The default parameters (n_samples / n_features / n_components) should make the example runnable in a couple of tens of seconds. You can try to increase the dimensions of the problem, but be aware that the time complexity is polynomial in NMF. In LDA, the time complexity is proportional to (n_samples * iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "41a0fcb0-f330-4839-9212-62f6aeddb356"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e5f6ba2d-bd2e-4faf-a8da-26d91003bf58"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
