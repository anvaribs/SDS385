# Final Project: 
## Matrix Factorization methods and their applications: Probabilistic, Non-negative, and sparse matrix factorization

In the final project, my goal is to study and implement some of the most popular and useful matrix factorization methods from scratch and benchmark their performance. In the write-up, I will start by talking about the matrix factorization in general and then I move onto Non-negative matrix factorization (NMF) and how it allows for "parts-based" learning from data and topic modeling. I will show two general objective functions that can be minimized to yield NMF factorization, namely **1. Squared error objective** and **2. beta-divergence objective.** 

Then I will talk about multiplicative update rule and the algorithm which can be used to minimize these objective functions and implement those in the simplest form. I will also talk about a more general regularized form of the objective functions, that is used in the most advanced ML libraries such as Sklearn and NIMFA. I will also implemented a sparse NMF method to compare it's performance with the state-of-the-art algorithms.

For the purpose of completion, and as a good direction for future work in this project, I added a short part about Principal Component Analysis (PCA) and it's variants. Specifically, **probabilistic PCA**, **Kermenl PCA**, and **Sparse PCA**. I will try to implement and benchmark some of these algorithms as well.

### Due date: December 16, 2017  - 11:59 PM

**Note:** I am doing all my write-up in a dropbox paper during the project. Below, you can find the link to the dropbox paper that I am working on. At the end of the project, I will also add a link to the pdf of the write-up as well.

**Note:** Please feel free to leave comments in my dropbox paper. I am always open to learn and implement new methods and algorithms.


link to the project documentation: https://paper.dropbox.com/doc/present/CFpZe33Gl4StoIsPXwZxd

[link to the playground that I am testing different methods](Playground_NMF.ipynb)
