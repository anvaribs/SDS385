# Exercise 3

## Exercises 3: Better online learning (preliminaries)

[The goal in this exercise](exercises03-SDS385.pdf) is to set the stage for some big improvements to
stochastic gradient descent.  To do so, we'll need to revisit our two batch optimizers from before: ordinary gradient
descent, and Newton's method.  These exercises will have you implement backtracking line search and the BFGS method for
logistic regression.  Both of these ideas will carry forward, with some modifications, to the online-learning setting.  

