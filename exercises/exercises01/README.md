## Excercises 1 ##

This directory hosts all the files related to the first homework.

I will add all the related details to this document. However, all the solutions would go into the following files directly:
* [exercises01-SDS385.tex](exercises01-SDS385.tex)
* [exercises01-SDS385.pdf](exercises01-SDS385.pdf)


 ## Exercises 1: Preliminaries
 
 The goal of this warm-up assignment is to provide you with an introduction to optimization and its role in data       analysis.
 
 For this section, please read Chapter 2 of _Numerical Optimization_, by Nocedal and Wright.  The full text of this    book is available for free through the [UT Library website](http://lib.utexas.edu).  You should come away with a      good general understanding of two methods for optimizing smooth functions:
 1) the method of steepest descent, or simply _gradient descent_, and
 2) Newton's method.
 
 Feel free to skip the stuff about trust-region methods.  The overview of quasi-Newton methods is nice, but optional   for now.
 
 The [exercises for this unit](exercises01/exercises01-SDS385.pdf) will have you practice these techniques.  They      will also will hammer your linear algebra skills.
